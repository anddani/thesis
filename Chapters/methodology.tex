\textit{To ensure that the experiment is carried out correctly, many different tools for measurements was evaluated. Different implementations of the FFT are also compared to choose the ones that would typically be used in an Android project.}
% Test how large the cost of JNI
% Debug/VMDebug libraries

% Preparation
% What tools are used? Phone, software, libraries (currenttimemillis vs nantotime etc),
% Different thread pools
% Measure execution time
% Measure memory consumption
% Benchmark Environment (Which phone, Android version, Compiler version, Java version, compiler flags)
% Benchmark Environment parameters (Started apps, memory left, started processes? cpu throttling by mobile case?)
% Profilers for time and memory measurement
% Disable Instant Run??
% Which operations should be included?
% Should I Measure every operation in detail?
% Garbage collection discussion (You cannot control it)
\section{Experiment model}

% http://lessthanoptimal.github.io/Java-Matrix-Benchmark/manual/MethodologyRuntimeBenchmark/
% In java, we cannot control GC, which can lead to varying results
% In java, do not use string concatenation (will ask for memory)
% Measure first, optimize later
This experiment consisted of tests for three different aspects of implementing algorithms in Java and in native code. To get an overview of how much of an impact different parts of an implementation have, the following subjects were investigated:

\begin{enumerate}
    \item Cost of using the JNI
    \item Compare well known libraries
    \item Compare two optimized code samples in Java and C++
\end{enumerate}

The reason why it is relevant to know how significant the JNI is, is because we want to see for what size of the data the transition time for going between Java and native is irrelevant compared to the total execution time of the JNI call. This would also show how much repeated calls to native code would affect the performance of a program. By minimizing the number of calls to the JNI, a program would get potentially faster.

There are many different implementations of the FFT publicly available that might be of interest for use in ones project. This test demonstrates how some different libraries compare. It is helpful to see how viable different implementations are on Android, for both C++ libraries and Java libraries. It can also be useful to know how a small implementation compares to a large and complex library.

Finally, comparing optimization techniques for small libraries is a good way of demonstrating how a developer can improve performance to fit the requirements while still retaining manageable source code. Having one single source file is valuable, especially for native libraries.

\subsection{Hardware}
% Phone model
% Screen Brightness
% Battery percentage?
% The Google Nexus 6P was used throughout all experiments to get comparable results. The screen brightness was set to medium to prevent heat from affecting the CPU frequency.
The setup used for performing the experiments were the following:

\begin{table}[H]
    \centering
    \begin{tabular}{|l|l|}
        \hline
        \textbf{Phone model} & Google Nexus 6P\\
        \hline
        \textbf{CPU model} & Qualcomm MSM8994 Snapdragon 810\\
        \hline
        \textbf{Core frequency} & 4x2.0 GHz and 4x1.55 GHz\\
        \hline
        \textbf{Total RAM} & 3 GB\\
        \hline
        \textbf{Available RAM} & 1.5 GB\\
        \hline
    \end{tabular}
\end{table}

\subsection{Benchmark Environment}
% Android version
% Cellular on?
% Wifi OFF!
% Location OFF
% Which apps were in background?
% Foreground services OFF
During the tests, cellular was switched off and wifi was enabled and connected. There were no applications running in the background while performing the tests during the experiments. The tests were executed and compiled with the following versions:

% All the tests were executed/compiled using . Cellular was switched off and wifi was connected. No other background apps were present while performing the tests during the experiments.
\begin{table}[H]
    \centering
    \begin{tabular}{|l|l|}
        \hline
        \textbf{Android version} & 7.1.1\\
        \hline
        \textbf{Kernel version} & 3.10.73g7196b0d\\
        \hline
        \textbf{Clang/LLVM version} & 3.8.256229\\
        \hline
        \textbf{Java version} & 1.8.0\_76\\
        \hline
    \end{tabular}
\end{table}


\subsection{Time measurement}
There are multiple methods of measuring time in Java. It is possible to measure the wall-clock time using the \texttt{System.currentTimeMillis()} method. There are drawback of using wall-clock time for measuring time. Because it can be changed at any time, it could result in too small or too large runtime depending on seemingly random factors. What is more preferable is to measure elapsed cpu time. This do not depend on a changeable wall clock but rather use hardware to interpret time. It is possible to use both \texttt{System.nanoTime()} and \texttt{SystemClock.elapsedRealtimeNanos()} for this purpose.

To get comparable results, matching work between algorithms were included in the time measurements. Each test was given the formatted data it needed because . Although every test had to return a complex representation of the results. They all described this using the same class called \emph{Complex} found in Appendix~\ref{appendix:code} Listing~\ref{lst:complex}. This class was written by Robert Sedgewick and Kevin Wayne \cite{princeton:complex}.

\subsection{Memory measurement}
The profiling tool provided by Android Studio was used to measure the amount of memory each test required. The method used was to attach the debugger to the app and measure using the profiler. To measure each test separately and equally, the app was launched freshly between tests and the garbage collector was forced before each test. After this, the memory allocation tracker was activated and then followed by starting a test. When the test had been done executing, the tracker was stopped and the results saved.

\section{Evaluation}
% How will the results be represented? 
% How will I interpret the results
% How many times will the programs be executed
% Statistical significance
% What is included in the calculation, what is not (adding result to string etc)
The unit of the resulting data will be in milliseconds. To be able to have 100 executions run in reasonable time, the maximum size of the input data was limited to $2^{16} = 65536$. The sampling rate is what determines the highest frequency that could be found in the result. In this thesis, only the frequency range perceivable by the human ear ($\sim$ 20-22,000 Hz) is covered by the tests. Because the FFT is limited to sample sizes of powers of 2, the next power of 2 for a sampling rate of 44,100 is $2^{16}$.

% INCORRECT?
% This size is also appropriate because a typical sample size in human audible sound processing is 44.1~kHz if you want to have a maximum frequency output of $\sim22$ kHz.


\subsection{Data representation}
\subsection{Data interpretation}
\subsection{Statistical significance}
Because the execution times differ between runs, it is important to calculate the sample mean. This way we have an expected value to use in our results. To get an accurate sample mean, we must have a large sample size which is the number of runs we execute for each test. The following formula calculates the sample mean \cite[p.263]{olofsson2012probability}:

\begin{equation*}
    \bar{X} = \frac{1}{N} \sum\limits_{k = 1}^{N} X_k
\end{equation*}

We cannot say anything about how close to our mean the samples are with only the sample mean. Therefore, the standard deviation is needed to find the dispersion of the data for each test. The standard deviation for a set of random samples $X_1, \dots, X_N$ is calculated using the following formula \cite[p.~302]{olofsson2012probability}:
\begin{equation*}
    s = \sqrt{\frac{1}{N - 1} \sum\limits_{k = 1}^{N}\left(X_k - \bar{X}\right)^2}
\end{equation*}

When comparing results, we need to find a confidence interval for a given test and choose a confidence level. For the data gathered in this study, a 95\% two-sided confidence level was chosen when comparing the data. The confidence interval is calculated by taking the standard error of the mean which is found by using the following formula \cite[p.~304]{olofsson2012probability}:
\begin{equation*}
    SE_{\bar{X}} = \frac{s}{\sqrt{N}}
\end{equation*}

When finding the confidence interval by taking the appropriate $z^*$-value for a confidence level and multiplying it with the standard error. For a confidence level of 95\%, we get an interval as follows:
\begin{equation*}
    \bar{X} \pm SE_{\bar{X}} \cdot 1.96
\end{equation*}

% Description of which algorithms that are available, which one that is used and why
% Detailed description/comparison between the code.
% Correctness test/verification??
% Complexity
% Test data: sizes of data, datatypes (long vs int vs float vs double)

\section{JNI Tests}
For testing the JNI overhead, three different tests were constructed. The first test had no parameters, returned void and did no calculations. The purpose of this test was to see how long it would take to call the smallest function possible. The function shown in Figure~\ref{fig:jni:empty} was used to test this.

\begin{figure}
\begin{lstlisting}[
        language={C++},
        basicstyle=\ttfamily\footnotesize,
        keywordstyle=\color{blue}\ttfamily,
        stringstyle=\color{red}\ttfamily,
        commentstyle=\color{green!70!black}\ttfamily,
    ]
                 void jniEmpty(JNIEnv*, jobject) {
                     return;
                 }
\end{lstlisting}
\caption{JNI test function with no parameters and no return value}
\label{fig:jni:empty}
\end{figure}

% TODO: JNI test with only params
For the second test, a function was written (see Figure~\ref{fig:jni:params}) that took a jdoubleArray as input and output. The reason this test was made was to see if JNI introduced some extra overhead for passing an argument and having a return value.

\begin{figure}
\begin{lstlisting}[
        language={C++},
        basicstyle=\ttfamily\footnotesize,
        keywordstyle=\color{blue}\ttfamily,
        stringstyle=\color{red}\ttfamily,
        commentstyle=\color{green!70!black}\ttfamily,
    ]
     jdoubleArray jniParams(JNIEnv*, jobject, jdoubleArray arr) {
         return arr;
     }
\end{lstlisting}
\caption{JNI test function with a double array as input parameter and return value}
\label{fig:jni:params}
\end{figure}

% TODO: JNI tests for making an array accessible in C++
In Figure~\ref{fig:jni:conversion}, the third test started by calling the \texttt{GetDoubleArrayElements} function to be able to access the elements stored in \texttt{arr}. When all the calculations are done, the function will return \texttt{arr}. To overwrite the changes made on \texttt{elements}, a function called \texttt{ReleaseDoubleArrayElements} must be called.

\begin{figure}
\begin{lstlisting}[
        language={C++},
        basicstyle=\ttfamily\footnotesize,
        keywordstyle=\color{blue}\ttfamily,
        stringstyle=\color{red}\ttfamily,
        commentstyle=\color{green!70!black}\ttfamily,
    ]
jdoubleArray jniVectorConversion(JNIEnv* env, jobject, jdoubleArray arr) {
    jdouble* elements = (*env).GetDoubleArrayElements(arr, 0);
    (*env).ReleaseDoubleArrayElements(arr, elements, 0);
    return arr;
}
\end{lstlisting}
\caption{Get and release elements}
\label{fig:jni:conversion}
\end{figure}

\section{Fast Fourier Transform Algorithms}
% Small block size
% Large block size

\subsection{Java libraries}
JTransforms \cite{jtransforms:benchmark}

\subsection{C++ libraries}
% Flags: -funroll-loops
\cite{FFTW05}

% => http://edp.org/work/Construction.pdf <== OPTIMIZATION
