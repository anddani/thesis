\textit{The process of developing, how Android installs the app and how it runs it is explained in this chapter. Additionally, some basic knowledge of the Discrete Fourier Transform is required when discussing differences in FFT implementations.}

\section{Android SDK}
To allow developers to build Android apps, Google developed a Standard Development Kit (SDK) to facilitate the process of writing Android applications. The Android SDK software stack is described in Figure~\ref{fig:sdk}. The Linux kernel is at the base of the stack, handling the core functionality of the device. Detecting hardware interaction, process scheduling and memory allocation are examples of services provided by the kernel. The Hardware Abstraction Layer (HAL) is an abstraction layer above the device drivers. This allows the developer to interact with hardware independent on the type of device \cite{android:hal}.

The native libraries are low level libraries, written in C or C++, that handle functionality such as the Secure Sockets Layer (SSL) and Open GL \cite{komatineni2012pro}. Android Runtime (ART) features Ahead-Of-Time (AOT) compilation and Just-In-Time (JIT) compilation, garbage collection and debugging support \cite{android:sdk:stack}. This is where the Java code is being run and because of the debugging and garbage collection support, it is also beneficial for the developer to write applications against this layer.

The Java API Framework is the Java library you use when controlling the Android UI. It is the reusable code for managing activities, implementing data structures and designing the application. The System Application layer represents the functionality that allows a third-party app to communicate with other apps. Example of usable applications are email, calendar and contacts \cite{android:sdk:stack}.

All applications for Android are packaged in so called Android Packages (APK). These APKs are zipped archives that contain all the necessary resources required to run the app. Examples of such resources are the AndroidManifest.xml file, Dalvik executables, native libraries and other files the application depends on.

\begin{figure}
    \centering

    \begin{tikzpicture}[node distance=3pt,outer sep=0pt,
            blueb/.style={
                draw=white,
                fill=mybluei,
                rounded corners,
                text width=2.5cm,
                font={\sffamily\bfseries\color{white}},
                align=center,
                text height=12pt,
            text depth=9pt},
            greenb/.style={blueb,fill=mygreen},
            darkgreenb/.style={blueb,fill=mydarkgreen},
            redb/.style={blueb,fill=myred},
            greyb/.style={blueb,fill=mygrey},
            yellowb/.style={blueb,fill=myyellow},
        ]

        \node[label=center:{\sffamily\bfseries\color{white}System Applications},darkgreenb,minimum width=15.9cm] (SysApps) {};

        \node[label=center:{\sffamily\bfseries\color{white}Java API Framework},greenb,minimum width=15.9cm,below=of SysApps] (JAPI) {};

        \node[label=center:{\sffamily\bfseries\color{white}Native Libraries},greyb,below=of JAPI,minimum width=13cm,xshift=-1.45cm] (Nat) {};
        \node[label=center:{\sffamily\bfseries\color{white}ART},yellowb,right=of Nat] (Art) {};

        \widernode{Nat}{Art}{Hardware Abstraction Layer (HAL)}{Hal}

        \widernode[redb]{Hal}{Hal}{Linux Kernel}{RCP}
        \begin{pgfonlayer}{background}
            \draw[blueb,draw=black,fill=mybluei!40] 
                ([xshift=-\myframesep,yshift=3\myframesep]current bounding box.north west) 
                rectangle 
                ([xshift=\myframesep,yshift=-\myframesep]current bounding box.south east);
        \end{pgfonlayer}
        \node[font=\sffamily\itshape\color{black},above=of SysApps] {Android SDK Software Stack};
    \end{tikzpicture}
    \caption{Android SDK Software Stack \cite{android:sdk:stack}}
    \label{fig:sdk}
\end{figure}


\section{Dalvik Virtual Machine}
% Why is Dalvik Virtual Machine used?
Compiled Java code is executed on a virtual machine called the Java Virtual Machine (JVM). The reason for this is to allow compiled code to become portable. This way, every device, independent on architecture, with a JVM installed will be able to run the same code. The Android operating system is designed to be installed on many different devices \cite{android:os:devices}. Because of the many different devices, user applications would have to be compiled for all possible platforms it should work on. For this reason, Java bytecode is a choice when wanting to distribute compiled applications.

% The DVM uses \cite{dalvik:vm:tech}

The Dalvik Virtual Machine (DVM) is the VM initially used on Android. One difference between DVM and JVM is that the DVM uses a register-based architecture while the JVM uses a stack-based architecture. The most common virtual machine architecture is the stack-based \cite[p.~158]{craig2010virtual}. A stack-based architecture evaluates each expression directly on the stack and always have the last evaluated value on top of the stack. Thus, only a stack pointer is needed to find the next instruction on the stack.

Contrary to this, a register based virtual machine works more like a CPU. It uses a set of registers where it will place operands by fetching them from memory. One advantage of using register-based architecture is that fetching data between registers is faster than fetching or storing data onto the hardware stack. The biggest disadvantage of using register-based architecture is that the compilers must be more complex than for stack-based architecture. This is because the code generators must take register management into consideration \cite[p.~159-160]{craig2010virtual}.

The DVM is a virtual machine optimized for devices where resources are limited \cite{android:dalvik:internals}. The main focus of the DVM is to lower memory consumption and lower the number of instructions needed to fulfil a task. Using register-based architecture, it is possible to execute more virtual machine instructions compared to a stack-based architecture \cite{shi2008virtual}. 

% http://davidehringer.com/software/android/The_Dalvik_Virtual_Machine.pdf

% Why was it used?

% READ:
% http://www.android-app-developer.co.uk/android-app-development-docs/android-jit-compiler-androids-dalvik-vm.pdf
% https://developer.android.com/about/versions/nougat/android-7.0.html


\subsection{Dalvik Executables}
% What are dex files?
Dalvik executables, or dex files, are the files that Dalvik bytecode is stored. They are created by converting a Java class file to the dex format. They are of a different structure than Java class files. Some differences are the header types that describes the data. One example of the differences is the string constant fields that are present in the dex-file. % FIND REFERENCE FROM GOOGLE IO ABOUT DALVIK??

% How do they compare to class files

\section{Android Runtime}
% What is Android Runtime?
Android Runtime is the new default runtime for Android as of version 5.0 \cite{android:dalvik}. The big improvement over Dalvik is the fact that applications compiled to binary when they are installed on the device, rather than during runtime of the app. This results in faster start-up \cite{li2016advanced} lets the compiler use more heavy optimization that is not otherwise possible during runtime. However, if the whole application is compiled ahead of time it is no longer possible to do any runtime optimizations. An example of a runtime optimization is to inline methods or functions that are called frequently.

% .oat file
When an app is installed on the device, a program called \textbf{dex2oat} converts a dex-file to an executable file called an oat-file \cite{android:art:dalvik}. This oat-file is in the Executable and Linkable Format (ELF) and can be seen as a wrapper of multiple dex-files \cite{Dresel2016}.

% Why is it used instead of Dalvik?

\section{Native Development Kit}
Native development kit (\gls{ndk}) is a set of tools to help writing native apps for Android. It contains the necessary libraries, compiler, build tools and debugger for developing low level libraries. Google recommends using the NDK for two reasons: run computationally intensive tasks and usage of already written libraries \cite{android:ndk:guides}. Because Java is the supported language on Android, due to security and stability, native development is not recommended to use to build full apps, with exception of for example games.

Historically, native libraries have been built using makefile. Makefile is a tool used to coordinate compilation of source files. Android makefiles, \texttt{Android.mk} and \texttt{Application.mk}, are used to set compiler flags, choose which architectures that should be compiled for, location of source files and more. With Android Studio 2.2 \gls{cmake} was introduced as the default build tool \cite{android:studio:cmake}. CMake is a more advanced tool for generating and running build scripts.

\subsection{Java Native Interface}
To be able to call native libraries from Java code, a framework named Java Native Interface (JNI) is used. Using this interface, C/C++ functions are mapped as methods and primitive data types are converted between Java and C/C++. For this to work, special syntax is needed for JNI to recognize which method in which class a native function should correspond to.

To mark a function as native in Java, a special keyword called \texttt{native} is used to define a method. The library which implements this method must also be included in the same class. By using the \texttt{System.loadLibrary("mylib")} call, we can specify the name of the shared object that should be loaded. Inside the native library we must follow a function naming convention to map a method to a function. The rules are that you must start the function with \texttt{Java} followed by the package, class and method name. Figure~\ref{fig:native} demonstrates how to map a method to a native function.

\begin{figure}
\begin{center}
    \texttt{private native int myFun();}\\
    $\Updownarrow$
    \begin{verbatim}
      JNIEXPORT jint JNICALL
      Java_com_example_MainActivity_myFun (JNIEnv *env, jobject thisObj)
    \end{verbatim}
\end{center}
\caption{Native method declaration to implementation.}
\label{fig:native}
\end{figure}

The JNI also provides a library for C and C++ for handling the special JNI data types. They can be used to determine the size of a Java array, get position of elements of an array and handling Java objects. In C and C++ you are given a pointer to a struct with all the required functions to convert data back and forth between Java data and C/C++ data.

\subsection{LLVM and Clang}
LLVM (Low Level Virtual Machine) is a suite that contains a set of compiler optimizers and backends. It is used as a foundation for compiler frontends and supports many architectures. An example of a frontend tool that uses LLVM is \gls{clang}. Clang is used compile C, C++, Objective-C \cite{clang:comp}.

Clang is as of March 2016 (NDK version 11) \cite{android:ndk:revision}, the only supported compiler in the NDK. Google has chosen to focus on a supporting the Clang compiler instead of the GNU GCC compiler. This means that there is a bigger chance that a specific architecture is supported in the NDK.


\section{Discrete Fourier Transform}
The Discrete Fourier Transform (\gls{dft}) is a method of converting a sampled signal from the time domain to the frequency domain. In other words, the DFT takes an observed signal and dissects each component that would form the observed signal. Every component of a signal can each be described as a sinusoidal wave with a frequency, amplitude and phase.

If we observe Figure~\ref{fig:dft:ex1}, we can see how a signal in time domain looks like in frequency domain. Function displayed in the time domain consists of three sin components, each with its own amplitude and frequency. What the graph of the frequency domain shows, is the amplitude of each frequency. This can then be used to analyze the input signal.

One important thing to note is that you must sample at twice the frequency you want to analyze. The Nyquist sampling theorem states that \cite{signal:aliasing}:
\begin{center}
    \textit{The sampling frequency should be at least twice the highest frequency contained in the signal.}
\end{center}
In other words, you have to be able to reconstruct the signal given the samples \cite[Ch~3]{smith1997scientist}. If you are given a signal that is constructed of signals that are at most 500 Hz, your sample frequency must be at least 1000 samples per second.

\begin{figure}[h]
    \renewcommand\thesubfigure{(\alph{subfigure})}
    \centering
    \begin{tikzpicture}
        \begin{groupplot}[group style={group name=my plots,group size= 1 by 2,horizontal sep =1.5cm,vertical sep =2cm},width=6cm]
            \nextgroupplot[
                legend pos=north east,
                trig format plots=rad,
                scaled ticks=false,
                tick label style={/pgf/number format/fixed},
                ymin=-4,
                ymax=4,
                xmin=0,
                xmax=1,
                ylabel={Amplitude},
                xlabel={Time $\left[s\right]$},
                width=14cm,
                height=5cm,
            ]
            \addlegendentry{$f(x) = 0.5\sin(10x) + \sin(20x) + 1.5\sin(30x)$}
            \addplot[domain=-0.5*pi:2*pi,samples=800,red] {0.5*sin(10*x) + sin(20*x) + 1.5*sin(30*x)};
            \nextgroupplot[
                legend pos=north east,
                trig format plots=rad,
                ymin=0,
                ymax=2.0,
                xmin=0,
                xmax=50,
                ylabel={Amplitude},
                xlabel={Frequency $\left[Hz\right]$},
                width=14cm,
                height=5cm,
                yshift=-0.5cm,
            ]
            \addplot[fill=blue,ybar] coordinates {
                    (10,0.5)
                    (20,1.0)
                    (30,1.5)
                };
            % \addplot[color=blue,mark=none] file {Data/frequencydomain.dat};
        \end{groupplot}
        \node[text width=12cm,align=center,anchor=north] at ([yshift=10mm]my plots c1r1.north) (cap1) {Time domain};
        \node[text width=12cm,align=center,anchor=north] at ([yshift=10mm]my plots c1r2.north) (cap2) {Frequency domain};
    \end{tikzpicture}
    \caption{Time domain $\Rightarrow$ Frequency domain}
    \label{fig:dft:ex1}
\end{figure}

Equation \ref{eq:1} \cite[p.~92]{tan2013digital} describes the mathematical process of converting a signal $x$ to a spectrum $X$ of $x$ where $N$ is the number of samples, $n$ is the time step and $k$ is the frequency sample. When calculating $X\left(k\right) \forall\ k \in \{x \in \mathbb{N} \mid 0 \leq x \leq N - 1\}$ we clearly see that it will take $N^2$ multiplications. In 1965, Cooley and Tukey published a paper on an algorithm that could calculate the DFT in less than $2N\log(N)$ multiplications \cite{Cooley1964} called the Fast Fourier Transform (FFT).

\begin{align}
    X\left(k\right) = \sum\limits_{n=0}^{N-1}x\left(n\right)\cdot e^{-j2\pi kn/N},\ \ k = 0,1,2,\dots,N-1\label{eq:1}
\end{align}

\section{Code Optimization}

There are many ways your compiler can optimize your code during compilation This chapter first present some general optimization measures taken by the optimizer and will then describe some language specific methods for optimization.

% Inlining
% Loop unrolling
\subsubsection{Loop unrolling}
Loop unrolling is a technique used to optimize loops. By explicitly coding multiple iterations in the body of the loop, it is possible to lower the amount of jump instructions in the produced code. Figure~\ref{fig:c:unroll} following example demonstrates how unrolling works. The loop unroll executes two iterations of the first code per iteration. It is therefore necessary to update the \texttt{i} variable accordingly. Figure~\ref{fig:assembly:unroll} describes how the change could be represented in assembly language.

\begin{figure}[H]
    \centering
    \begin{subfigure}{.5\textwidth}
        \centering
        \begin{verbatim}
    for (int i = 0; i < 6; ++i) {
        a[i] = a[i] + b[i];
    }

        \end{verbatim}
        \caption{Normal}
        \label{fig:c:unroll:normal}
    \end{subfigure}%
    \begin{subfigure}{.5\textwidth}
        \centering
        \begin{verbatim}
    for (int i = 0; i < 6; i+=2) {
        a[i] = a[i] + b[i];
        a[i+1] = a[i+1] + b[i+1];
    }
        \end{verbatim}
        \caption{One unroll}
        \label{fig:c:unroll:unroll}
    \end{subfigure}
    \caption{Loop unrolling in C}
    \label{fig:c:unroll}
\end{figure}

The gain in using loop unrolling is that you \enquote{save} the same amount of jump instructions as the amount of \enquote{hard coded} iterations you add. In theory, it is also possible to optimize even more by changing the offset of \texttt{LOAD WORD} instructions as shown in Figure~\ref{fig:assembly:optimized}. Then you would not need to update the iterator as often.


\begin{figure}
    \centering
    \begin{adjustbox}{minipage=\linewidth,scale=0.8}
        \begin{verbatim}
                $s1 - a[] address | $s4 - value of a[x]
                $s2 - b[] address | $s5 - value of b[x]
                $s3 - i           | $s6 - value 6
        \end{verbatim}
        \begin{subfigure}{.55\textwidth}
            \centering
            \begin{lstlisting}[
                    language={[mips]Assembler},
                    basicstyle=\footnotesize,
                    numbers=left,
                    firstnumber=1,
                    numberfirstline=true
                ]
loop: lw $s4, 0($s1) # Load a[i]
      lw $s5, 0($s2) # Load b[i]
      add $s4, $s4, $s5 # a[i] + b[i]
      sw $s4, 0($s1)
      addi $s1, $s1, 4 # next element
      addi $s2, $s2, 4 # next element
      addi $s3, $s3, 1 # i++
      bge $s3, $s6, loop
                \end{lstlisting}
            \caption{Normal}
            \label{fig:unroll:sub1}
        \end{subfigure}%
        \begin{subfigure}{.3\textwidth}
            \centering
            \begin{lstlisting}[
                    language={[mips]Assembler},
                    basicstyle=\footnotesize,
                    numbers=left,
                    firstnumber=1,
                    numberfirstline=true
                ]
loop: lw $s4, 0($s1)
      lw $s5, 0($s2)
      add $s4, $s4, $s5
      sw $s4, 0($s1)
      addi $s1, $s1, 4
      addi $s2, $s2, 4
      addi $s3, $s3, 1
      lw $s4, 0($s1)
      lw $s5, 0($s2)
      add $s4, $s4, $s5
      sw $s4, 0($s1)
      addi $s1, $s1, 4
      addi $s2, $s2, 4
      addi $s3, $s3, 1
      bge $s3, $s6, loop
                \end{lstlisting}
            \caption{One unroll}
            \label{fig:unroll:sub2}
        \end{subfigure}
    \end{adjustbox}
    \caption{Loop unrolling in assembly}
    % https://www.cs.umd.edu/class/fall2001/cmsc411/proj01/proja/why.html
    \label{fig:assembly:unroll}
\end{figure}


\subsubsection{Inlining}
Inlining allows the compiler to swap all the calls to an inline function with the content of the function. This removes the need to do all the preparations for a function call such as saving values in registers, preparing parameters and return values. This comes at a 

% TODO: Set side by side
\begin{figure}[h]
    \centering
    \begin{adjustbox}{minipage=\linewidth,scale=0.8}
        \centering
        \begin{verbatim}
                  $s1 - a[] address | $s4 - value of a[x]
                  $s2 - b[] address | $s5 - value of b[x]
                  $s3 - i           | $s6 - value 6
        \end{verbatim}
        \begin{subfigure}{.55\textwidth}
            \centering
            \begin{lstlisting}[
                    language={[mips]Assembler},
                    basicstyle=\footnotesize,
                    numbers=left,
                    firstnumber=1,
                    numberfirstline=true
                ]
loop: lw $s4, 0($s1)
      lw $s5, 0($s2)
      add $s4, $s4, $s5
      sw $s4, 0($s1)
      addi $s1, $s1, 4
      addi $s2, $s2, 4
      addi $s3, $s3, 1
      lw $s4, 0($s1)
      lw $s5, 0($s2)
      add $s4, $s4, $s5
      sw $s4, 0($s1)
      addi $s1, $s1, 4
      addi $s2, $s2, 4
      addi $s3, $s3, 1
      bge $s3, $s6, loop
                \end{lstlisting}
            \caption{One unroll}
            \label{fig:optimized:sub1}
        \end{subfigure}%
        \begin{subfigure}{.3\textwidth}
            \centering
            \begin{lstlisting}[
                    language={[mips]Assembler},
                    basicstyle=\footnotesize,
                    numbers=left,
                    firstnumber=1,
                    numberfirstline=true
                ]
loop: lw $s4, 0($s1)
      lw $s5, 0($s2)
      add $s4, $s4, $s5
      sw $s4, 0($s1)
      lw $s4, 4($s1)
      lw $s5, 4($s2)
      add $s4, $s4, $s5
      sw $s4, 4($s1)
      addi $s1, $s1, 8
      addi $s2, $s2, 8
      addi $s3, $s3, 2
      bge $s3, $s6, loop
                \end{lstlisting}
            \caption{Optimized unroll}
            \label{fig:optimized:sub2}
        \end{subfigure}
    \end{adjustbox}
    \caption{Optimized loop unrolling in assembly}
    % https://www.cs.umd.edu/class/fall2001/cmsc411/proj01/proja/why.html
    \label{fig:assembly:optimized}
\end{figure}

cost of a larger program if there are many calls to this function in the code. It is very useful to have inline functions in loops that are run many times. This is an optimization that can be used manually in C and C++ using the \texttt{inline} keyword and can also be optimized by the compiler.

\subsubsection{Constant folding}
\emph{Constant folding} is a technique used to reduce the time it takes to evaluate an expression in runtime \cite[p.~329]{muchnick1997advanced}. By finding which variables that already have a value, the compiler can calculate and assign constants in compile time instead of runtime. This method of analyzing the code to find expressions consisting of variables that are possible to calculate is called \emph{Constant Propagation} as seen in Figure~\ref{fig:constant:propagation}.

\begin{figure}[h]
    \centering
    \begin{adjustbox}{minipage=\linewidth,scale=1.0}
        \begin{subfigure}{.40\textwidth}
            \centering
            \begin{lstlisting}[
                    language={C},
                    basicstyle=\footnotesize,
                ]
      int x = 10;
      int y = x * 5 + 3;
                \end{lstlisting}
            \caption{Before optimization}
            \label{fig:propagation:sub1}
        \end{subfigure}%
        \begin{subfigure}{.50\textwidth}
            \centering
            \begin{lstlisting}[
                    language={C},
                    basicstyle=\footnotesize,
                ]
              int x = 10;
              int y = 53;
                \end{lstlisting}
            \caption{Constant propagation optimization}
            \label{fig:propagation:sub2}
        \end{subfigure}
    \end{adjustbox}
    \caption{Constant Propagation}
    \label{fig:constant:propagation}
\end{figure}

\subsubsection{Loop Tiling}
When processing elements in a large array multiple times it is beneficial to utilize as many reads from cache as possible. If the array is larger than the cache, it will kick out earlier elements for the next pass through the array. By processing partitions of the array multiple times before going on to next partition, temporal cache locality can help the program run faster. Temporal locality means that you can find a previously referenced value in the cache if you are trying to access it again. As Figure~\ref{fig:loop:tiling} shows, by introducing a new loop that operate over a small enough partition of the array such that every element is in cache, we will reduce the number of cache misses.

\begin{figure}[h]
    \begin{adjustbox}{minipage=\linewidth,scale=0.8}
        \begin{subfigure}{.50\textwidth}
            \centering
            \begin{lstlisting}[
                    language={C},
                    basicstyle=\footnotesize,
                ]
for (i = 0; i < NUM_REPS; ++i) {
    for (j = 0; j < ARR_SIZE; ++j) {
        a[j] = a[j] * 17;
    }
}
                \end{lstlisting}
            \caption{Before loop tiling}
            \label{fig:tiling:sub1}
        \end{subfigure}%
        \begin{subfigure}{.50\textwidth}
            \centering
            \begin{lstlisting}[
                    language={C},
                    basicstyle=\footnotesize,
                ]
    for (j = 0; j < ARR_SIZE; j += 1024) {
        for (i = 0; i < NUM_REPS; ++i) {
            for (k = j; k < (j + 1024); ++k) {
                a[k] = a[k] * 17;
            }
        }
    }
                \end{lstlisting}
            \caption{After loop tiling}
            \label{fig:tiling:sub2}
        \end{subfigure}
    \end{adjustbox}
    \caption{Loop Tiling}
    \label{fig:loop:tiling}
\end{figure}


% http://www.azillionmonkeys.com/qed/optimize.html
% => Optimizing compilers for modern architectures <=

% \subsubsection{Loop fission and loop fusion}
% \emph{Loop Fusion} and \emph{Loop Fission} are two ways of changing how loops are executed. They are the opposites of themselves and are used based on the instructions in the loop. The purpose of \emph{Loop Fission} is to separate a statement or statements from each other and have multiple loops. This increases the loop overhead but can become more efficient if the separated loop has good cache locality. As we see in Figure~\ref{fig:fission:sub1}, we read from two arrays, \texttt{b} and \texttt{c}. We cannot know if they are close to each other in memory which can lead to many cache misses every iteration of the loop. If we split the loop as in Figure~\ref{fig:fission:sub2}, we can utilize memory locality to get more efficient code.
%
% \begin{figure}[h]
%     \centering
%     \begin{subfigure}{.45\textwidth}
%         \centering
%         \begin{lstlisting}[
%                 language={C},
%                 basicstyle=\footnotesize,
%                 numbers=left,
%                 firstnumber=1,
%                 numberfirstline=true
%             ]
% for (i = 0; i < 10; ++i) {
%     a[i] = b[i];
%     b[i] = c[i];
% }
%             \end{lstlisting}
%         \caption{Before split}
%         \label{fig:fission:sub1}
%     \end{subfigure}%
%     \begin{subfigure}{.35\textwidth}
%         \centering
%         \begin{lstlisting}[
%                 language={C},
%                 basicstyle=\footnotesize,
%                 numbers=left,
%                 firstnumber=1,
%                 numberfirstline=true
%             ]
% for (i = 0; i < 10; ++i)
%     a[i] = b[i];
% for (i = 0; i < 10; ++i)
%     b[i] = c[i];
%         \end{lstlisting}
%         \caption{After split}
%         \label{fig:fission:sub2}
%     \end{subfigure}
%     \caption{Loop fission}
%     \label{fig:c:loopfission}
% \end{figure}
%
% On the other hand, \emph{Loop Fusion} merges two loops into one because memory locality is possible in one loop. This removes the overhead of having two loops. Because both assignments require reads from the \texttt{a} array that are close to each other, cache locality is possible. Both of these techniques are commonly used by the optimizer in the compiler.
%
% \begin{figure}[h]
%     \centering
%     \begin{subfigure}{.45\textwidth}
%         \centering
%         \begin{lstlisting}[
%                 language={C},
%                 basicstyle=\footnotesize,
%                 numbers=left,
%                 firstnumber=1,
%                 numberfirstline=true
%             ]
% for (i = 1; i < 10; ++i)
%     a[i] = a[i-1];
% for (i = 1; i < 10; ++i)
%     b[i] = a[i];
%             \end{lstlisting}
%         \caption{Before fusion}
%         \label{fig:fusion:sub1}
%     \end{subfigure}%
%     \begin{subfigure}{.35\textwidth}
%         \centering
%         \begin{lstlisting}[
%                 language={C},
%                 basicstyle=\footnotesize,
%                 numbers=left,
%                 firstnumber=1,
%                 numberfirstline=true
%             ]
% for (i = 1; i < 10; ++i) {
%     a[i] = a[i-1];
%     b[i] = a[i];
% }
%         \end{lstlisting}
%         \caption{After fusion}
%         \label{fig:fusion:sub2}
%     \end{subfigure}
%     \caption{Loop fusion}
%     \label{fig:c:loopfusion}
% \end{figure}

\subsection{Java}
In Java, an array is created during runtime and cannot change its size after it is created. This means that it will always be placed on the heap and the garbage collector will handle the memory it resides on should be released. By keeping an array reference in scope and reusing the same array, we can circumvent this behaviour and save some instructions by not needing to ask for more memory from the heap.

% Object allocation pool
% CPU usage during runtime
% Garbage collection
% http://www.cubrid.org/blog/dev-platform/understanding-java-garbage-collection/
% Java Performance: The Definitive Guide
% http://www.gamasutra.com/view/news/128161/A_Low_Level_Curriculum_for_C_and_C.php
% https://lwn.net/Articles/250967/
% Hacker's Delight (2nd Edition)
% https://en.wikibooks.org/wiki/C++_Programming/Optimization
% http://www.slideshare.net/cdman83/performance-optimization-techniques-for-java-code
% http://www.agner.org/optimize/optimizing_cpp.pdf
% http://leto.net/docs/C-optimization.php
% http://www.appperfect.com/support/java-coding-rules/optimization.php
% https://developer.android.com/training/articles/perf-tips.html#UseFinal
% http://blog.takipi.com/garbage-collectors-serial-vs-parallel-vs-cms-vs-the-g1-and-whats-new-in-java-8/
% http://stackoverflow.com/questions/9546392/what-triggers-a-full-garbage-collection-in-java
% https://www.itu.dk/people/sestoft/papers/benchmarking.pdf

\subsection{C++}
In C and C++, arrays with predefined sizes are located on the program stack. This makes the program run faster because it does not need to call malloc or new and ask for more memory on the heap. This require that the programmer knows the required size of the array in advance and is not always possible or memory efficient.

% agner.org/optimize

\subsubsection{NEON}
% https://www.kernel.org/pub/linux/kernel/people/geoff/cell/ps3-linux-docs/CellProgrammingTutorial/BasicsOfSIMDProgramming.html

% http://www.crickettechnology.com/blog/?p=691
% http://wanderingcoder.net/2010/07/19/ought-arm/
% http://wanderingcoder.net/2010/06/02/intro-neon/
Android NDK includes a tool called NEON, containing functions that enable Single Instruction Multiple Data (SIMD). SIMD is an efficient way of executing the same type of operation on multiple operands at the same time. Figure~\ref{fig:simd} describes this concept where instead of operating on one piece of data at the time, a larger set of data that uses the same operation can be processed with one operation.

NEON provides a set of functions compatible with the ARM architecture. These functions can perform operations on double word and quad word registers. The reason you would want to use SIMD because you can have instructions that loads blocks of multiple values and operates on these blocks.  The process starts by reading the data into larger vector registers, operate on these registers and storing the results as blocks \cite{simd:expl}. This way you will have less instructions than if you loaded one element at a time and operated on only that value.

SIMD has some prerequisites on the data that is being processed. Firstly, the data blocks must line up meaning that you cannot operate between two operands that are not in the same area of the block. Secondly, all the operands of a block must be of the same type.

\begin{figure}
    \centering
    \begin{subfigure}{.45\textwidth}
        \centering
        \begin{tikzpicture}[
                node distance = 0mm,
                start chain=1 going {below=of \tikzchainprevious.south west}, 
                box/.style = {draw, semithick, minimum size=1cm, every on chain/.style={anchor=north west}, outer sep = 0mm, on chain}
            ]
            \foreach \i in {1,...,4}
            {
                \pgfmathtruncatemacro{\y}{\i - 1};
                \pgfmathtruncatemacro{\lab}{(\i - 4) * -1}
                \node[fill=myeggshell,box] (A\lab) at (0, 1.5*\y) {A$_\lab$};
                \node[fill=mylightgreen,box] (B\lab) at (1.6, 1.5*\y) {B$_\lab$};
                \node[fill=mylightred,box] (C\lab) at (4, 1.5*\y) {C$_\lab$};
                \node[fill=none,right of=A\lab,node distance=0.8cm] (plus\i) {+};
                \node[fill=none,right of=B\lab,node distance=1.15cm] (equals\i) {=};
            };
        \end{tikzpicture}
        \caption{Four separate instructions}
        \label{fig:simd:simd1}
    \end{subfigure}%
    \begin{subfigure}{.35\textwidth}
        \centering
        \begin{tikzpicture}[
                node distance = 0mm,
                start chain=1 going {below=of \tikzchainprevious.south west}, 
                box/.style = {draw, semithick, minimum size=1cm, every on chain/.style={anchor=north west}, outer sep = 0mm, on chain}
            ]
            \foreach \i in {1,...,4}
            {
                \pgfmathtruncatemacro{\y}{\i - 1};
                \pgfmathtruncatemacro{\lab}{(\i - 4) * -1}
                \node[fill=myeggshell,box] (A\lab) at (0, 1*\y) {A$_\lab$};
                \node[fill=mylightgreen,box] (B\lab) at (1.6, 1*\y) {B$_\lab$};
                \node[fill=mylightred,box] (C\lab) at (4, 1*\y) {C$_\lab$};
            };
            \node[fill=none] (plus) at (1.3, 0.95) {+};
            \node[fill=none] (equals) at (3.3, 0.95) {=};
        \end{tikzpicture}
        \caption{One instruction with SIMD}
        \label{fig:simd:simd2}
    \end{subfigure}
    \caption{Single Instruction Multiple Data\cite{kernel:simd}}
    \label{fig:simd}
\end{figure}

\section{Related work}
A study called \emph{FFT benchmark on Android devices: Java versus JNI} \cite{Jr2013} was carried out in 2013 and investigated how two implementations of FFT performed on different Android devices. The main point of the study was to compare how a pure Java implementation would perform compared to a library written in C called FFTW. This library supports multi-threaded computation and this aspect is also covered in this study. Their benchmark application was run on 35 different devices with different versions to get a wide picture of how the algorithms ran on different phones.

\emph{Evaluating Performance of Android Platform Using Native C for Embedded Systems} \cite{Lee2010} explored how JNI overhead, arithmetic operations, memory access and heap allocation affected an application written in Java and native C. It was written in 2010 when the Android NDK was relatively new. Since then, many patches has been released, improving performance of code written in native C/C++. In this study, Dalvik VM was the virtual machine that executed the Dalvik bytecode. This study found that the JNI overhead was insignificant and took 0.15 ms to run in their testing environment. Their test results indicated that C was faster than Java in every case. The performance difference was largest in the memory access test and smallest in floating point calculations.

In 2016, \emph{Android App Energy Efficiency: The Impact of Language, Runtime, Compiler, and Implementation} \cite{Chen2016} presented a performance comparison between ART and native on Android. The main focus of the report was to find how much more efficient one of them were in terms of energy consumption. Their tests consisted of measuring battery drainage in power as well as execution time of different algorithms. It also compares performance differences between ART and Dalvik. The conclusion states that native performs much better than code running on the Dalvik VM. However, code compiled by ART improves greatly from Dalvik and is comparable with code compiled in Android NDK.
